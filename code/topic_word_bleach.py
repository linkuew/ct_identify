#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""


Created on: 2023-10-03
Author: zytian9


"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from sklearn.decomposition import LatentDirichletAllocation
import argparse

# Constants
SEEDS = ['big.foot', 'flat.earth', 'climate', 'vaccine', 'pizzagate']
STOP_WORDS = set(list(stopwords.words("english") + 
                     ["you", "it", "we", "there", "they", "us", "the", "them", "i've", "i'm", "too", "you've"]))
FOLDER = '../data/'

def display_topics(model, feature_names, no_top_words):
    """
    Display the topics generated by the LDA model.
    
    Parameters:
    - model: Trained LDA model.
    - feature_names: List of feature names from the vectorizer.
    - no_top_words: Number of top words to display for each topic.
    """
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic {topic_idx + 1}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))

def save_topics_to_file(model, feature_names, no_top_words):
    """
    Save the topics generated by the LDA model to a file.
    
    Parameters:
    - model: Trained LDA model.
    - feature_names: List of feature names from the vectorizer.
    - no_top_words: Number of top words to save for each topic.
    
    Returns:
    - List of unique top words across all topics.
    """
    num_topics = model.n_components
    top_words = []
    
    with open(f"{num_topics}_topic_words.txt", 'w') as file:
        file.write(f"Number of Topics: {num_topics}\n\n")
        for topic_idx, topic in enumerate(model.components_):
            words = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]
            file.write(f"Topic {topic_idx + 1}:\n{' '.join(words)}\n\n")
            top_words.extend(words)
    
    return list(set(top_words))

def bleach_topic_words(topic_words, df):
    """
    Replace words in df['word_pos'] with their POS if they appear in topic_words.
    
    Parameters:
    - topic_words: List of words to be replaced.
    - df: DataFrame containing 'word_pos' column.
    
    Returns:
    - Modified DataFrame.
    """
    bleach_lines = []
    
    for v in df['word_pos'].values:
        new_doc = []
        for item in v.split():
            word, pos = item.split('_')
            new_doc.append(pos if word.lower() in topic_words else word)
        bleach_lines.append(' '.join(new_doc))
    
    df['bleach'] = bleach_lines
    return df

def gen_new_df(folder, n_topics, topic_words):
    """
    Generate new CSV files with bleached topic words.
    
    Parameters:
    - folder: Folder where CSV files are located.
    - n_topics: Number of topics used in LDA modeling.
    - topic_words: List of words to be replaced in the bleaching process.
    """
    for data_type in ['train', 'test', 'val']:
        for seed in SEEDS:
            file = f"{folder}{data_type}_{seed}.wp.csv"
            df = pd.read_csv(file)
            df = bleach_topic_words(topic_words, df)
            df.drop(['text'], axis=1, inplace=True)
            df.drop(df.columns[0], axis=1, inplace=True)
            df.rename(columns={'bleach': 'text'}, inplace=True)
            df.to_csv(f"{folder}{data_type}_{seed}.tp{n_topics}.csv")

if __name__ == '__main__':
    # Argument parsing
    parser = argparse.ArgumentParser(description="LDA Topic Modeling Script")
    parser.add_argument("--n_topics", type=int, default=10, help="Number of topics for LDA (default: 10)")
    parser.add_argument("--no_top_words", type=int, default=20, help="Number of top words to display/save for each topic (default: 20)")
    args = parser.parse_args()

    # Read and merge data
    dfs = [pd.read_csv(f'{FOLDER}train_{s}.csv') for s in SEEDS]
    merged_df = pd.concat(dfs, ignore_index=True)
    merged_df['text'] = merged_df['text'].str.lower()

    # Vectorize data
    vectorizer = CountVectorizer(stop_words=list(STOP_WORDS), min_df=2)
    X = vectorizer.fit_transform(merged_df['text'])

    # LDA modeling
    lda_model = LatentDirichletAllocation(n_components=args.n_topics, random_state=42)
    lda_model.fit(X)

    # Save topics and generate new dataframes
    topic_words = save_topics_to_file(lda_model, vectorizer.get_feature_names_out(), args.no_top_words)
    gen_new_df(FOLDER, args.n_topics, topic_words)


