big.foot
big.foot
(1064, 247977)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  85.71428571428571
              precision    recall  f1-score   support

  conspiracy     0.8710    0.6429    0.7397        42
  mainstream     0.8529    0.9560    0.9016        91

    accuracy                         0.8571       133
   macro avg     0.8620    0.7995    0.8206       133
weighted avg     0.8586    0.8571    0.8505       133

{'conspiracy': {'precision': 0.8709677419354839, 'recall': 0.6428571428571429, 'f1-score': 0.7397260273972603, 'support': 42}, 'mainstream': {'precision': 0.8529411764705882, 'recall': 0.9560439560439561, 'f1-score': 0.9015544041450777, 'support': 91}, 'accuracy': 0.8571428571428571, 'macro avg': {'precision': 0.861954459203036, 'recall': 0.7994505494505495, 'f1-score': 0.820640215771169, 'support': 133}, 'weighted avg': {'precision': 0.8586337760910815, 'recall': 0.8571428571428571, 'f1-score': 0.8504507062247143, 'support': 133}}
big.foot
flat.earth
(1064, 247977)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  74.43609022556392
              precision    recall  f1-score   support

  conspiracy     0.5102    0.7143    0.5952        35
  mainstream     0.8810    0.7551    0.8132        98

    accuracy                         0.7444       133
   macro avg     0.6956    0.7347    0.7042       133
weighted avg     0.7834    0.7444    0.7558       133

{'conspiracy': {'precision': 0.5102040816326531, 'recall': 0.7142857142857143, 'f1-score': 0.5952380952380952, 'support': 35}, 'mainstream': {'precision': 0.8809523809523809, 'recall': 0.7551020408163265, 'f1-score': 0.8131868131868131, 'support': 98}, 'accuracy': 0.7443609022556391, 'macro avg': {'precision': 0.695578231292517, 'recall': 0.7346938775510203, 'f1-score': 0.7042124542124542, 'support': 133}, 'weighted avg': {'precision': 0.7833870390261367, 'recall': 0.7443609022556391, 'f1-score': 0.7558318874108346, 'support': 133}}
big.foot
climate
(1064, 247977)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  74.43609022556392
              precision    recall  f1-score   support

  conspiracy     0.4681    0.7097    0.5641        31
  mainstream     0.8953    0.7549    0.8191       102

    accuracy                         0.7444       133
   macro avg     0.6817    0.7323    0.6916       133
weighted avg     0.7958    0.7444    0.7597       133

{'conspiracy': {'precision': 0.46808510638297873, 'recall': 0.7096774193548387, 'f1-score': 0.5641025641025641, 'support': 31}, 'mainstream': {'precision': 0.8953488372093024, 'recall': 0.7549019607843137, 'f1-score': 0.8191489361702127, 'support': 102}, 'accuracy': 0.7443609022556391, 'macro avg': {'precision': 0.6817169717961405, 'recall': 0.7322896900695762, 'f1-score': 0.6916257501363884, 'support': 133}, 'weighted avg': {'precision': 0.7957610503249712, 'recall': 0.7443609022556391, 'f1-score': 0.7597020374176029, 'support': 133}}
big.foot
vaccine
(1064, 247977)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  78.94736842105263
              precision    recall  f1-score   support

  conspiracy     0.5641    0.6667    0.6111        33
  mainstream     0.8830    0.8300    0.8557       100

    accuracy                         0.7895       133
   macro avg     0.7235    0.7483    0.7334       133
weighted avg     0.8039    0.7895    0.7950       133

{'conspiracy': {'precision': 0.5641025641025641, 'recall': 0.6666666666666666, 'f1-score': 0.611111111111111, 'support': 33}, 'mainstream': {'precision': 0.8829787234042553, 'recall': 0.83, 'f1-score': 0.8556701030927835, 'support': 100}, 'accuracy': 0.7894736842105263, 'macro avg': {'precision': 0.7235406437534098, 'recall': 0.7483333333333333, 'f1-score': 0.7333906071019473, 'support': 133}, 'weighted avg': {'precision': 0.8038590748557154, 'recall': 0.7894736842105263, 'f1-score': 0.7949900524507143, 'support': 133}}
big.foot
pizzagate
(1064, 247977)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  63.1578947368421
              precision    recall  f1-score   support

  conspiracy     0.4177    0.9167    0.5739        36
  mainstream     0.9444    0.5258    0.6755        97

    accuracy                         0.6316       133
   macro avg     0.6811    0.7212    0.6247       133
weighted avg     0.8019    0.6316    0.6480       133

{'conspiracy': {'precision': 0.4177215189873418, 'recall': 0.9166666666666666, 'f1-score': 0.5739130434782609, 'support': 36}, 'mainstream': {'precision': 0.9444444444444444, 'recall': 0.5257731958762887, 'f1-score': 0.6754966887417219, 'support': 97}, 'accuracy': 0.631578947368421, 'macro avg': {'precision': 0.6810829817158931, 'recall': 0.7212199312714777, 'f1-score': 0.6247048661099914, 'support': 133}, 'weighted avg': {'precision': 0.801872825523725, 'recall': 0.631578947368421, 'f1-score': 0.6480003637080032, 'support': 133}}
flat.earth
big.foot
(1064, 243722)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  81.203007518797
              precision    recall  f1-score   support

  conspiracy     0.7429    0.6190    0.6753        42
  mainstream     0.8367    0.9011    0.8677        91

    accuracy                         0.8120       133
   macro avg     0.7898    0.7601    0.7715       133
weighted avg     0.8071    0.8120    0.8070       133

{'conspiracy': {'precision': 0.7428571428571429, 'recall': 0.6190476190476191, 'f1-score': 0.6753246753246753, 'support': 42}, 'mainstream': {'precision': 0.8367346938775511, 'recall': 0.9010989010989011, 'f1-score': 0.8677248677248677, 'support': 91}, 'accuracy': 0.8120300751879699, 'macro avg': {'precision': 0.789795918367347, 'recall': 0.76007326007326, 'f1-score': 0.7715247715247715, 'support': 133}, 'weighted avg': {'precision': 0.8070891514500538, 'recall': 0.8120300751879699, 'f1-score': 0.8069669122300701, 'support': 133}}
flat.earth
flat.earth
(1064, 243722)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  89.47368421052632
              precision    recall  f1-score   support

  conspiracy     0.8889    0.6857    0.7742        35
  mainstream     0.8962    0.9694    0.9314        98

    accuracy                         0.8947       133
   macro avg     0.8926    0.8276    0.8528       133
weighted avg     0.8943    0.8947    0.8900       133

{'conspiracy': {'precision': 0.8888888888888888, 'recall': 0.6857142857142857, 'f1-score': 0.7741935483870968, 'support': 35}, 'mainstream': {'precision': 0.8962264150943396, 'recall': 0.9693877551020408, 'f1-score': 0.9313725490196079, 'support': 98}, 'accuracy': 0.8947368421052632, 'macro avg': {'precision': 0.8925576519916143, 'recall': 0.8275510204081633, 'f1-score': 0.8527830487033523, 'support': 133}, 'weighted avg': {'precision': 0.8942954871455369, 'recall': 0.8947368421052632, 'f1-score': 0.8900096541163155, 'support': 133}}
flat.earth
climate
(1064, 243722)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  83.45864661654136
              precision    recall  f1-score   support

  conspiracy     0.6667    0.5806    0.6207        31
  mainstream     0.8774    0.9118    0.8942       102

    accuracy                         0.8346       133
   macro avg     0.7720    0.7462    0.7575       133
weighted avg     0.8282    0.8346    0.8305       133

{'conspiracy': {'precision': 0.6666666666666666, 'recall': 0.5806451612903226, 'f1-score': 0.6206896551724138, 'support': 31}, 'mainstream': {'precision': 0.8773584905660378, 'recall': 0.9117647058823529, 'f1-score': 0.8942307692307693, 'support': 102}, 'accuracy': 0.8345864661654135, 'macro avg': {'precision': 0.7720125786163522, 'recall': 0.7462049335863378, 'f1-score': 0.7574602122015915, 'support': 133}, 'weighted avg': {'precision': 0.8282498699579136, 'recall': 0.8345864661654135, 'f1-score': 0.8304730659540097, 'support': 133}}
flat.earth
vaccine
(1064, 243722)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  77.44360902255639
              precision    recall  f1-score   support

  conspiracy     0.5366    0.6667    0.5946        33
  mainstream     0.8804    0.8100    0.8438       100

    accuracy                         0.7744       133
   macro avg     0.7085    0.7383    0.7192       133
weighted avg     0.7951    0.7744    0.7819       133

{'conspiracy': {'precision': 0.5365853658536586, 'recall': 0.6666666666666666, 'f1-score': 0.5945945945945946, 'support': 33}, 'mainstream': {'precision': 0.8804347826086957, 'recall': 0.81, 'f1-score': 0.84375, 'support': 100}, 'accuracy': 0.7744360902255639, 'macro avg': {'precision': 0.7085100742311772, 'recall': 0.7383333333333333, 'f1-score': 0.7191722972972974, 'support': 133}, 'weighted avg': {'precision': 0.7951187619100774, 'recall': 0.7744360902255639, 'f1-score': 0.7819294858768543, 'support': 133}}
flat.earth
pizzagate
(1064, 243722)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  85.71428571428571
              precision    recall  f1-score   support

  conspiracy     0.7297    0.7500    0.7397        36
  mainstream     0.9062    0.8969    0.9016        97

    accuracy                         0.8571       133
   macro avg     0.8180    0.8235    0.8206       133
weighted avg     0.8585    0.8571    0.8578       133

{'conspiracy': {'precision': 0.7297297297297297, 'recall': 0.75, 'f1-score': 0.7397260273972601, 'support': 36}, 'mainstream': {'precision': 0.90625, 'recall': 0.8969072164948454, 'f1-score': 0.9015544041450777, 'support': 97}, 'accuracy': 0.8571428571428571, 'macro avg': {'precision': 0.8179898648648649, 'recall': 0.8234536082474226, 'f1-score': 0.820640215771169, 'support': 133}, 'weighted avg': {'precision': 0.8584700772200773, 'recall': 0.8571428571428571, 'f1-score': 0.857751234499052, 'support': 133}}
climate
big.foot
(1064, 227065)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  75.18796992481202
              precision    recall  f1-score   support

  conspiracy     0.5738    0.8333    0.6796        42
  mainstream     0.9028    0.7143    0.7975        91

    accuracy                         0.7519       133
   macro avg     0.7383    0.7738    0.7386       133
weighted avg     0.7989    0.7519    0.7603       133

{'conspiracy': {'precision': 0.5737704918032787, 'recall': 0.8333333333333334, 'f1-score': 0.6796116504854368, 'support': 42}, 'mainstream': {'precision': 0.9027777777777778, 'recall': 0.7142857142857143, 'f1-score': 0.7975460122699387, 'support': 91}, 'accuracy': 0.7518796992481203, 'macro avg': {'precision': 0.7382741347905282, 'recall': 0.7738095238095238, 'f1-score': 0.7385788313776878, 'support': 133}, 'weighted avg': {'precision': 0.7988807401016202, 'recall': 0.7518796992481203, 'f1-score': 0.7603035822327275, 'support': 133}}
climate
flat.earth
(1064, 227065)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  64.66165413533834
              precision    recall  f1-score   support

  conspiracy     0.4167    0.8571    0.5607        35
  mainstream     0.9180    0.5714    0.7044        98

    accuracy                         0.6466       133
   macro avg     0.6673    0.7143    0.6326       133
weighted avg     0.7861    0.6466    0.6666       133

{'conspiracy': {'precision': 0.4166666666666667, 'recall': 0.8571428571428571, 'f1-score': 0.5607476635514019, 'support': 35}, 'mainstream': {'precision': 0.9180327868852459, 'recall': 0.5714285714285714, 'f1-score': 0.7044025157232705, 'support': 98}, 'accuracy': 0.6466165413533834, 'macro avg': {'precision': 0.6673497267759563, 'recall': 0.7142857142857142, 'f1-score': 0.6325750896373362, 'support': 133}, 'weighted avg': {'precision': 0.786094334196146, 'recall': 0.6466165413533834, 'f1-score': 0.6665986072569893, 'support': 133}}
climate
climate
(1064, 227065)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  88.7218045112782
              precision    recall  f1-score   support

  conspiracy     0.7667    0.7419    0.7541        31
  mainstream     0.9223    0.9314    0.9268       102

    accuracy                         0.8872       133
   macro avg     0.8445    0.8367    0.8405       133
weighted avg     0.8860    0.8872    0.8866       133

{'conspiracy': {'precision': 0.7666666666666667, 'recall': 0.7419354838709677, 'f1-score': 0.7540983606557377, 'support': 31}, 'mainstream': {'precision': 0.9223300970873787, 'recall': 0.9313725490196079, 'f1-score': 0.9268292682926829, 'support': 102}, 'accuracy': 0.8872180451127819, 'macro avg': {'precision': 0.8444983818770226, 'recall': 0.8366540164452878, 'f1-score': 0.8404638144742103, 'support': 133}, 'weighted avg': {'precision': 0.8860476433802954, 'recall': 0.8872180451127819, 'f1-score': 0.8865686807983574, 'support': 133}}
climate
vaccine
(1064, 227065)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  75.93984962406014
              precision    recall  f1-score   support

  conspiracy     0.5098    0.7879    0.6190        33
  mainstream     0.9146    0.7500    0.8242       100

    accuracy                         0.7594       133
   macro avg     0.7122    0.7689    0.7216       133
weighted avg     0.8142    0.7594    0.7733       133

{'conspiracy': {'precision': 0.5098039215686274, 'recall': 0.7878787878787878, 'f1-score': 0.6190476190476191, 'support': 33}, 'mainstream': {'precision': 0.9146341463414634, 'recall': 0.75, 'f1-score': 0.8241758241758242, 'support': 100}, 'accuracy': 0.7593984962406015, 'macro avg': {'precision': 0.7122190339550454, 'recall': 0.7689393939393939, 'f1-score': 0.7216117216117217, 'support': 133}, 'weighted avg': {'precision': 0.8141875492173762, 'recall': 0.7593984962406015, 'f1-score': 0.7732793522267207, 'support': 133}}
climate
pizzagate
(1064, 227065)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  51.127819548872175
              precision    recall  f1-score   support

  conspiracy     0.3535    0.9722    0.5185        36
  mainstream     0.9706    0.3402    0.5038        97

    accuracy                         0.5113       133
   macro avg     0.6621    0.6562    0.5112       133
weighted avg     0.8036    0.5113    0.5078       133

{'conspiracy': {'precision': 0.35353535353535354, 'recall': 0.9722222222222222, 'f1-score': 0.5185185185185186, 'support': 36}, 'mainstream': {'precision': 0.9705882352941176, 'recall': 0.3402061855670103, 'f1-score': 0.5038167938931297, 'support': 97}, 'accuracy': 0.5112781954887218, 'macro avg': {'precision': 0.6620617944147356, 'recall': 0.6562142038946163, 'f1-score': 0.5111676562058242, 'support': 133}, 'weighted avg': {'precision': 0.80356640263761, 'recall': 0.5112781954887218, 'f1-score': 0.5077962080774455, 'support': 133}}
vaccine
big.foot
(1064, 209716)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  78.19548872180451
              precision    recall  f1-score   support

  conspiracy     0.6327    0.7381    0.6813        42
  mainstream     0.8690    0.8022    0.8343        91

    accuracy                         0.7820       133
   macro avg     0.7509    0.7701    0.7578       133
weighted avg     0.7944    0.7820    0.7860       133

{'conspiracy': {'precision': 0.6326530612244898, 'recall': 0.7380952380952381, 'f1-score': 0.6813186813186813, 'support': 42}, 'mainstream': {'precision': 0.8690476190476191, 'recall': 0.8021978021978022, 'f1-score': 0.8342857142857143, 'support': 91}, 'accuracy': 0.7819548872180451, 'macro avg': {'precision': 0.7508503401360545, 'recall': 0.7701465201465202, 'f1-score': 0.7578021978021978, 'support': 133}, 'weighted avg': {'precision': 0.7943967060508413, 'recall': 0.7819548872180451, 'f1-score': 0.7859803354540197, 'support': 133}}
vaccine
flat.earth
(1064, 209716)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  79.69924812030075
              precision    recall  f1-score   support

  conspiracy     0.5870    0.7714    0.6667        35
  mainstream     0.9080    0.8061    0.8541        98

    accuracy                         0.7970       133
   macro avg     0.7475    0.7888    0.7604       133
weighted avg     0.8235    0.7970    0.8047       133

{'conspiracy': {'precision': 0.5869565217391305, 'recall': 0.7714285714285715, 'f1-score': 0.6666666666666667, 'support': 35}, 'mainstream': {'precision': 0.9080459770114943, 'recall': 0.8061224489795918, 'f1-score': 0.854054054054054, 'support': 98}, 'accuracy': 0.7969924812030075, 'macro avg': {'precision': 0.7475012493753124, 'recall': 0.7887755102040817, 'f1-score': 0.7603603603603604, 'support': 133}, 'weighted avg': {'precision': 0.8235487519398196, 'recall': 0.7969924812030075, 'f1-score': 0.8047415836889522, 'support': 133}}
vaccine
climate
(1064, 209716)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  78.94736842105263
              precision    recall  f1-score   support

  conspiracy     0.5385    0.6774    0.6000        31
  mainstream     0.8936    0.8235    0.8571       102

    accuracy                         0.7895       133
   macro avg     0.7160    0.7505    0.7286       133
weighted avg     0.8108    0.7895    0.7972       133

{'conspiracy': {'precision': 0.5384615384615384, 'recall': 0.6774193548387096, 'f1-score': 0.6, 'support': 31}, 'mainstream': {'precision': 0.8936170212765957, 'recall': 0.8235294117647058, 'f1-score': 0.8571428571428571, 'support': 102}, 'accuracy': 0.7894736842105263, 'macro avg': {'precision': 0.7160392798690671, 'recall': 0.7504743833017078, 'f1-score': 0.7285714285714285, 'support': 133}, 'weighted avg': {'precision': 0.8108364200189508, 'recall': 0.7894736842105263, 'f1-score': 0.7972073039742212, 'support': 133}}
vaccine
vaccine
(1064, 209716)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  91.72932330827066
              precision    recall  f1-score   support

  conspiracy     0.8438    0.8182    0.8308        33
  mainstream     0.9406    0.9500    0.9453       100

    accuracy                         0.9173       133
   macro avg     0.8922    0.8841    0.8880       133
weighted avg     0.9166    0.9173    0.9169       133

{'conspiracy': {'precision': 0.84375, 'recall': 0.8181818181818182, 'f1-score': 0.8307692307692308, 'support': 33}, 'mainstream': {'precision': 0.9405940594059405, 'recall': 0.95, 'f1-score': 0.9452736318407959, 'support': 100}, 'accuracy': 0.9172932330827067, 'macro avg': {'precision': 0.8921720297029703, 'recall': 0.884090909090909, 'f1-score': 0.8880214313050134, 'support': 133}, 'weighted avg': {'precision': 0.9165650822601056, 'recall': 0.9172932330827067, 'f1-score': 0.9168627654095053, 'support': 133}}
vaccine
pizzagate
(1064, 209716)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  75.18796992481202
              precision    recall  f1-score   support

  conspiracy     0.5263    0.8333    0.6452        36
  mainstream     0.9211    0.7216    0.8092        97

    accuracy                         0.7519       133
   macro avg     0.7237    0.7775    0.7272       133
weighted avg     0.8142    0.7519    0.7648       133

{'conspiracy': {'precision': 0.5263157894736842, 'recall': 0.8333333333333334, 'f1-score': 0.6451612903225806, 'support': 36}, 'mainstream': {'precision': 0.9210526315789473, 'recall': 0.7216494845360825, 'f1-score': 0.8092485549132947, 'support': 97}, 'accuracy': 0.7518796992481203, 'macro avg': {'precision': 0.7236842105263157, 'recall': 0.7774914089347079, 'f1-score': 0.7272049226179377, 'support': 133}, 'weighted avg': {'precision': 0.8142065690542144, 'recall': 0.7518796992481203, 'f1-score': 0.7648339569789661, 'support': 133}}
pizzagate
big.foot
(1064, 264857)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  79.69924812030075
              precision    recall  f1-score   support

  conspiracy     0.7143    0.5952    0.6494        42
  mainstream     0.8265    0.8901    0.8571        91

    accuracy                         0.7970       133
   macro avg     0.7704    0.7427    0.7532       133
weighted avg     0.7911    0.7970    0.7915       133

{'conspiracy': {'precision': 0.7142857142857143, 'recall': 0.5952380952380952, 'f1-score': 0.6493506493506493, 'support': 42}, 'mainstream': {'precision': 0.826530612244898, 'recall': 0.8901098901098901, 'f1-score': 0.8571428571428572, 'support': 91}, 'accuracy': 0.7969924812030075, 'macro avg': {'precision': 0.7704081632653061, 'recall': 0.7426739926739927, 'f1-score': 0.7532467532467533, 'support': 133}, 'weighted avg': {'precision': 0.7910848549946294, 'recall': 0.7969924812030075, 'f1-score': 0.7915242652084757, 'support': 133}}
pizzagate
flat.earth
(1064, 264857)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  72.18045112781954
              precision    recall  f1-score   support

  conspiracy     0.4667    0.4000    0.4308        35
  mainstream     0.7961    0.8367    0.8159        98

    accuracy                         0.7218       133
   macro avg     0.6314    0.6184    0.6233       133
weighted avg     0.7094    0.7218    0.7146       133

{'conspiracy': {'precision': 0.4666666666666667, 'recall': 0.4, 'f1-score': 0.4307692307692308, 'support': 35}, 'mainstream': {'precision': 0.7961165048543689, 'recall': 0.8367346938775511, 'f1-score': 0.8159203980099503, 'support': 98}, 'accuracy': 0.7218045112781954, 'macro avg': {'precision': 0.6313915857605178, 'recall': 0.6183673469387756, 'f1-score': 0.6233448143895906, 'support': 133}, 'weighted avg': {'precision': 0.7094191790154998, 'recall': 0.7218045112781954, 'f1-score': 0.7145648276834451, 'support': 133}}
pizzagate
climate
(1064, 264857)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  71.42857142857143
              precision    recall  f1-score   support

  conspiracy     0.4146    0.5484    0.4722        31
  mainstream     0.8478    0.7647    0.8041       102

    accuracy                         0.7143       133
   macro avg     0.6312    0.6565    0.6382       133
weighted avg     0.7469    0.7143    0.7268       133

{'conspiracy': {'precision': 0.4146341463414634, 'recall': 0.5483870967741935, 'f1-score': 0.4722222222222222, 'support': 31}, 'mainstream': {'precision': 0.8478260869565217, 'recall': 0.7647058823529411, 'f1-score': 0.8041237113402062, 'support': 102}, 'accuracy': 0.7142857142857143, 'macro avg': {'precision': 0.6312301166489925, 'recall': 0.6565464895635673, 'f1-score': 0.6381729667812142, 'support': 133}, 'weighted avg': {'precision': 0.7468565368883503, 'recall': 0.7142857142857143, 'f1-score': 0.726763213876616, 'support': 133}}
pizzagate
vaccine
(1064, 264857)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  72.18045112781954
              precision    recall  f1-score   support

  conspiracy     0.4643    0.7879    0.5843        33
  mainstream     0.9091    0.7000    0.7910       100

    accuracy                         0.7218       133
   macro avg     0.6867    0.7439    0.6876       133
weighted avg     0.7987    0.7218    0.7397       133

{'conspiracy': {'precision': 0.4642857142857143, 'recall': 0.7878787878787878, 'f1-score': 0.5842696629213484, 'support': 33}, 'mainstream': {'precision': 0.9090909090909091, 'recall': 0.7, 'f1-score': 0.7909604519774012, 'support': 100}, 'accuracy': 0.7218045112781954, 'macro avg': {'precision': 0.6866883116883117, 'recall': 0.7439393939393939, 'f1-score': 0.6876150574493748, 'support': 133}, 'weighted avg': {'precision': 0.7987257103798456, 'recall': 0.7218045112781954, 'f1-score': 0.7396762712341699, 'support': 133}}
pizzagate
pizzagate
(1064, 264857)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  93.98496240601504
              precision    recall  f1-score   support

  conspiracy     0.8889    0.8889    0.8889        36
  mainstream     0.9588    0.9588    0.9588        97

    accuracy                         0.9398       133
   macro avg     0.9238    0.9238    0.9238       133
weighted avg     0.9398    0.9398    0.9398       133

{'conspiracy': {'precision': 0.8888888888888888, 'recall': 0.8888888888888888, 'f1-score': 0.8888888888888888, 'support': 36}, 'mainstream': {'precision': 0.9587628865979382, 'recall': 0.9587628865979382, 'f1-score': 0.9587628865979382, 'support': 97}, 'accuracy': 0.9398496240601504, 'macro avg': {'precision': 0.9238258877434136, 'recall': 0.9238258877434136, 'f1-score': 0.9238258877434136, 'support': 133}, 'weighted avg': {'precision': 0.9398496240601504, 'recall': 0.9398496240601504, 'f1-score': 0.9398496240601504, 'support': 133}}
fe_cc_va_pg
big.foot
(1064, 246870)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  81.203007518797
              precision    recall  f1-score   support

  conspiracy     0.7297    0.6429    0.6835        42
  mainstream     0.8438    0.8901    0.8663        91

    accuracy                         0.8120       133
   macro avg     0.7867    0.7665    0.7749       133
weighted avg     0.8077    0.8120    0.8086       133

{'conspiracy': {'precision': 0.7297297297297297, 'recall': 0.6428571428571429, 'f1-score': 0.6835443037974683, 'support': 42}, 'mainstream': {'precision': 0.84375, 'recall': 0.8901098901098901, 'f1-score': 0.8663101604278075, 'support': 91}, 'accuracy': 0.8120300751879699, 'macro avg': {'precision': 0.7867398648648649, 'recall': 0.7664835164835164, 'f1-score': 0.7749272321126379, 'support': 133}, 'weighted avg': {'precision': 0.8077435988620199, 'recall': 0.8120300751879699, 'f1-score': 0.8085946267550689, 'support': 133}}
bf_cc_va_pg
flat.earth
(1064, 245657)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  80.45112781954887
              precision    recall  f1-score   support

  conspiracy     0.6364    0.6000    0.6176        35
  mainstream     0.8600    0.8776    0.8687        98

    accuracy                         0.8045       133
   macro avg     0.7482    0.7388    0.7432       133
weighted avg     0.8011    0.8045    0.8026       133

{'conspiracy': {'precision': 0.6363636363636364, 'recall': 0.6, 'f1-score': 0.6176470588235293, 'support': 35}, 'mainstream': {'precision': 0.86, 'recall': 0.8775510204081632, 'f1-score': 0.8686868686868686, 'support': 98}, 'accuracy': 0.8045112781954887, 'macro avg': {'precision': 0.7481818181818182, 'recall': 0.7387755102040816, 'f1-score': 0.743166963755199, 'support': 133}, 'weighted avg': {'precision': 0.8011483253588517, 'recall': 0.8045112781954887, 'f1-score': 0.8026237608280951, 'support': 133}}
bf_fe_va_pg
climate
(1064, 244833)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  84.21052631578947
              precision    recall  f1-score   support

  conspiracy     0.7778    0.4516    0.5714        31
  mainstream     0.8522    0.9608    0.9032       102

    accuracy                         0.8421       133
   macro avg     0.8150    0.7062    0.7373       133
weighted avg     0.8348    0.8421    0.8259       133

{'conspiracy': {'precision': 0.7777777777777778, 'recall': 0.45161290322580644, 'f1-score': 0.5714285714285714, 'support': 31}, 'mainstream': {'precision': 0.8521739130434782, 'recall': 0.9607843137254902, 'f1-score': 0.9032258064516129, 'support': 102}, 'accuracy': 0.8421052631578947, 'macro avg': {'precision': 0.814975845410628, 'recall': 0.7061986084756483, 'f1-score': 0.7373271889400921, 'support': 133}, 'weighted avg': {'precision': 0.834833460462751, 'recall': 0.8421052631578947, 'f1-score': 0.8258896088146633, 'support': 133}}
bf_fe_cc_pg
vaccine
(1064, 255971)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  75.18796992481202
              precision    recall  f1-score   support

  conspiracy     0.5000    0.7576    0.6024        33
  mainstream     0.9036    0.7500    0.8197       100

    accuracy                         0.7519       133
   macro avg     0.7018    0.7538    0.7110       133
weighted avg     0.8035    0.7519    0.7658       133

{'conspiracy': {'precision': 0.5, 'recall': 0.7575757575757576, 'f1-score': 0.6024096385542169, 'support': 33}, 'mainstream': {'precision': 0.9036144578313253, 'recall': 0.75, 'f1-score': 0.8196721311475409, 'support': 100}, 'accuracy': 0.7518796992481203, 'macro avg': {'precision': 0.7018072289156627, 'recall': 0.7537878787878788, 'f1-score': 0.7110408848508789, 'support': 133}, 'weighted avg': {'precision': 0.80346951716641, 'recall': 0.7518796992481203, 'f1-score': 0.7657648961431823, 'support': 133}}
bf_fe_cc_va
pizzagate
(1064, 237382)
Fitting 1 folds for each of 12 candidates, totalling 12 fits
{'C': 0.01, 'loss': 'squared_hinge', 'random_state': 1291}
SVM accuracy:  81.95488721804512
              precision    recall  f1-score   support

  conspiracy     0.6364    0.7778    0.7000        36
  mainstream     0.9101    0.8351    0.8710        97

    accuracy                         0.8195       133
   macro avg     0.7732    0.8064    0.7855       133
weighted avg     0.8360    0.8195    0.8247       133

{'conspiracy': {'precision': 0.6363636363636364, 'recall': 0.7777777777777778, 'f1-score': 0.7000000000000001, 'support': 36}, 'mainstream': {'precision': 0.9101123595505618, 'recall': 0.8350515463917526, 'f1-score': 0.870967741935484, 'support': 97}, 'accuracy': 0.8195488721804511, 'macro avg': {'precision': 0.7732379979570991, 'recall': 0.8064146620847652, 'f1-score': 0.7854838709677421, 'support': 133}, 'weighted avg': {'precision': 0.8360149607931985, 'recall': 0.8195488721804511, 'f1-score': 0.8246907591559546, 'support': 133}}
