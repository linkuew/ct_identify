{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c6f3ad",
   "metadata": {},
   "source": [
    "# Bleach topic words and do machine learning experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f8cfb",
   "metadata": {},
   "source": [
    "## LDA Topic Modeling using `scikit-learn` in Jupyter Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce0471",
   "metadata": {},
   "source": [
    "\n",
    "### What is Topic Modeling?\n",
    "\n",
    "Topic modeling is an unsupervised machine learning technique used to automatically identify topics present in a text corpus. By analyzing the patterns of words and their co-occurrence, topic modeling algorithms can group words into topics and assign topics to individual documents. This helps in understanding the main themes of large collections of texts without having to read through each document.\n",
    "\n",
    "### How does LDA work?\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is one of the most popular topic modeling techniques. At a high level, LDA works as follows:\n",
    "\n",
    "1. **Initialization**: Randomly assign each word in each document to a topic.\n",
    "2. **Iterative Assignment**:\n",
    "    - For each document, go through each word and reassign it to a topic based on:\n",
    "        a. How prevalent is that word across topics?\n",
    "        b. How prevalent are topics in that document?\n",
    "3. **Convergence**: The algorithm iteratively updates topic assignments until they stabilize and don't change much between iterations.\n",
    "\n",
    "The result is that each document gets a distribution over topics, and each topic gets a distribution over words.\n",
    "\n",
    "### What are the outcomes?\n",
    "\n",
    "The output of LDA can be visualized in two primary ways:\n",
    "\n",
    "1. **Document-Topic Distribution**: Each document is represented as a mixture of topics.\n",
    "2. **Topic-Word Distribution**: Each topic is represented as a mixture of words.\n",
    "\n",
    "For example, if we have a corpus of news articles, LDA might identify topics such as 'politics', 'sports', 'economy', etc. An article about a football match would have a high probability for the 'sports' topic, while an article about a new policy might have a high probability for the 'politics' topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d633377",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "We use the training data from all five conspiracies to generate topic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7c2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List all your CSV filenames\n",
    "seeds = ['big.foot','flat.earth','climate', 'vaccine','pizzagate']\n",
    "\n",
    "# Use a comprehension to read each file into a DataFrame\n",
    "dfs = [pd.read_csv(f'./data/train_{s}.csv') for s in seeds]\n",
    "\n",
    "# Concatenate all of these DataFrames\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68e871a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>seeds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>844</td>\n",
       "      <td>Sounds like a Dan Brown novel set in the Pacif...</td>\n",
       "      <td>mainstream</td>\n",
       "      <td>M108d5</td>\n",
       "      <td>big.foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>170</td>\n",
       "      <td>ST. PETERSBURG, Fla. — Even on a night when th...</td>\n",
       "      <td>mainstream</td>\n",
       "      <td>M10dee</td>\n",
       "      <td>big.foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>942</td>\n",
       "      <td>Our physical body if its human, needs its down...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>C01c06</td>\n",
       "      <td>big.foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>461</td>\n",
       "      <td>For the first time since Franklin D. Roosevelt...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>C0198f</td>\n",
       "      <td>big.foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>209</td>\n",
       "      <td>Imagine taking a helicopter to an uncharted is...</td>\n",
       "      <td>mainstream</td>\n",
       "      <td>M0c1b4</td>\n",
       "      <td>big.foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>1095</td>\n",
       "      <td>The conspiracy theory took the internet by sto...</td>\n",
       "      <td>mainstream</td>\n",
       "      <td>M07f3c</td>\n",
       "      <td>pizzagate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>1130</td>\n",
       "      <td>If QAnon’s claims were true, they would shake ...</td>\n",
       "      <td>mainstream</td>\n",
       "      <td>M0e932</td>\n",
       "      <td>pizzagate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>1294</td>\n",
       "      <td>On Sunday afternoon, a 28-year-old man walked ...</td>\n",
       "      <td>mainstream</td>\n",
       "      <td>M13dfb</td>\n",
       "      <td>pizzagate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>860</td>\n",
       "      <td>June 22 (Reuters) - A North Carolina man who w...</td>\n",
       "      <td>mainstream</td>\n",
       "      <td>M1520a</td>\n",
       "      <td>pizzagate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>1126</td>\n",
       "      <td>The story seems to encapsulate many of the ele...</td>\n",
       "      <td>mainstream</td>\n",
       "      <td>M1fc3c</td>\n",
       "      <td>pizzagate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5320 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  \\\n",
       "0            844  Sounds like a Dan Brown novel set in the Pacif...   \n",
       "1            170  ST. PETERSBURG, Fla. — Even on a night when th...   \n",
       "2            942  Our physical body if its human, needs its down...   \n",
       "3            461  For the first time since Franklin D. Roosevelt...   \n",
       "4            209  Imagine taking a helicopter to an uncharted is...   \n",
       "...          ...                                                ...   \n",
       "5315        1095  The conspiracy theory took the internet by sto...   \n",
       "5316        1130  If QAnon’s claims were true, they would shake ...   \n",
       "5317        1294  On Sunday afternoon, a 28-year-old man walked ...   \n",
       "5318         860  June 22 (Reuters) - A North Carolina man who w...   \n",
       "5319        1126  The story seems to encapsulate many of the ele...   \n",
       "\n",
       "           label  doc_id      seeds  \n",
       "0     mainstream  M108d5   big.foot  \n",
       "1     mainstream  M10dee   big.foot  \n",
       "2     conspiracy  C01c06   big.foot  \n",
       "3     conspiracy  C0198f   big.foot  \n",
       "4     mainstream  M0c1b4   big.foot  \n",
       "...          ...     ...        ...  \n",
       "5315  mainstream  M07f3c  pizzagate  \n",
       "5316  mainstream  M0e932  pizzagate  \n",
       "5317  mainstream  M13dfb  pizzagate  \n",
       "5318  mainstream  M1520a  pizzagate  \n",
       "5319  mainstream  M1fc3c  pizzagate  \n",
       "\n",
       "[5320 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5531d5cc",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "1. Stop words\n",
    "nltk default and a few added by Danny:\n",
    "\n",
    "2. Lowercase all the data\n",
    "\n",
    "Before performing LDA, we need to convert our documents into a matrix of token counts. We'll use CountVectorizer for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83da72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Lowercase the data\n",
    "\n",
    "merged_df['text'] = merged_df['text'].str.lower()\n",
    "\n",
    "# Stop words\n",
    "\n",
    "STOP_WORDS = set(list(stopwords.words(\"english\")+[\"you\", \"it\", \"we\", \"there\", \"they\", \"us\", \"the\", \"them\", \"i've\", \"i'm\", \"too\", \"you've\"]))\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=list(STOP_WORDS), min_df=2)\n",
    "\n",
    "# Transform the documents into a matrix of token counts\n",
    "X = vectorizer.fit_transform(merged_df['text'])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198373bf",
   "metadata": {},
   "source": [
    "## Build LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8be88dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=5, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=5, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Initialize LDA model with the desired number of topics\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "\n",
    "# Fit the LDA model on the data\n",
    "lda_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ace4dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "543d8dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_topics_to_file(model, feature_names, no_top_words):\n",
    "    num_topics = model.n_components\n",
    "    with open(f\"{num_topics}_topic_words.txt\", 'w') as file:\n",
    "        file.write(f\"Number of Topics: {num_topics}\\n\\n\")\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            file.write(f\"Topic {topic_idx + 1}:\\n\")\n",
    "            file.write(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "            file.write(\"\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b3be903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "one like earth people would time world know also even\n",
      "Topic 2:\n",
      "trump state new government president people said one american would\n",
      "Topic 3:\n",
      "climate change said global world emissions new carbon would countries\n",
      "Topic 4:\n",
      "vaccine vaccines people may health also children study one climate\n",
      "Topic 5:\n",
      "said news trump media conspiracy people one clinton also fake\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 10\n",
    "display_topics(lda_model, vectorizer.get_feature_names_out(), no_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb910156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save topic words to a file\n",
    "no_top_words = 20\n",
    "save_topics_to_file(lda_model, vectorizer.get_feature_names_out(), no_top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1865a9d",
   "metadata": {},
   "source": [
    "## Bleach words and generate new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34f00afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleach_topic_words(topic_words, df): \n",
    "    bleach_words = topic_words\n",
    "    bleach_lines = []\n",
    "    for v in df['word_pos'].values:\n",
    "        new_doc = []\n",
    "        try:\n",
    "            for i in v.split():\n",
    "                word = i.split('_')[0]\n",
    "                pos = i.split('_')[1]\n",
    "                if word.lower() in bleach_words:\n",
    "                    new_doc.append(pos)\n",
    "                else:\n",
    "                    new_doc.append(word)\n",
    "        except:\n",
    "            print (i)\n",
    "        bleach_lines.append(' '.join(new_doc))\n",
    "    \n",
    "    df['bleach'] = bleach_lines\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc2d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_df(folder,n_topics)\n",
    "    seeds = ['big.foot','flat.earth','climate', 'vaccine','pizzagate']\n",
    "    for data_type in ['train','test','val']:\n",
    "        for seed in seeds:\n",
    "            file = f\"{folder}{data_type}_{seed}.wp.csv\"\n",
    "            df = pd.read_csv(file)\n",
    "            df = topic_words(df)\n",
    "            df.drop(['text'], axis=1, inplace=True)\n",
    "            df.drop(df.columns[0], axis=1, inplace=True)\n",
    "            df.rename(columns={'bleach': 'text'}, inplace=True)\n",
    "            df.to_csv(f\"{folder}{data_type}_{seed}.tp{n_topics}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a196058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
